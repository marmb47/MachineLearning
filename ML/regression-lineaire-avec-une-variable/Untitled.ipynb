{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à remplir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Représentation du modèle et fonction d'erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser un modèle qui permet de prédire le profit d'un food-truck connaissant la nombre d'habitants de la ville ou il sera stationné. Pour se faire, nous disposons d'un dataset qui contient les profits de plusieurs food-truck stationnés sur plusieurs villes différents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Comprendre le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le dataset a été téléchargé depuis kaggle sur le lien https://www.kaggle.com/alphaepsilon/housing-prices-dataset/data\n",
    "Il s'agit de deux fichier csv (train.csv et test.csv) qui contienent une multitude de champs (81 dans train, 80 dans test) qui portent sur les informtions relatives au maisons. Chaque ligne représente une maison.\n",
    "Dans cet articles nous n'allons utiliser que 2 champs : \n",
    "- le champ représentant la surface de la maison : LotArea (qui se trouve à la 5 eme position).\n",
    "- le champ représentant le prix de la maison : SalePrice (qui se trouve à la derniére position)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, allons recupérer ces deux champs et voir à quoi ils resembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8450. 208500.]\n",
      " [  9600. 181500.]\n",
      " [ 11250. 223500.]\n",
      " [  9550. 140000.]\n",
      " [ 14260. 250000.]\n",
      " [ 14115. 143000.]\n",
      " [ 10084. 307000.]\n",
      " [ 10382. 200000.]\n",
      " [  6120. 129900.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tab = np.genfromtxt('train.csv', dtype=float, delimiter=',',skip_header=1)\n",
    "\n",
    "# recuperation des surfaces des maisons\n",
    "X = tab[:,4];\n",
    "\n",
    "# recuperation des prix des maisons\n",
    "Y = tab[:,80];\n",
    "\n",
    "dataset = np.ones((np.size(X),2))\n",
    "dataset[:,0] = X\n",
    "dataset[:,1] = Y\n",
    "\n",
    "# on affiche les 10 premiers maisons\n",
    "print(dataset[0:9,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a recupérer les surfaces des maisons X et leur prix correspondant Y qu'on a mis sur un tableau à deux colonnes qui se nomme \"dataset\".\n",
    "le prémière colonne représente les surface des maisons et la deuxieme colonne représente les prix. On nomme m le nombre d'exemples (de maisons) dans le dataset.\n",
    "On obtient ainsi l'ensemble $$E_m = \\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De ce faite $$ (x^{(i)}, y^{(i)}) $$ représente la i-ème ligne dela table \"dataset\" : la surface et le prix dala i-ème maison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si on visualise les 50 premiers élément du dataset, on aura : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(dataset[0:49,0], dataset[0:49,0], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Représenter le modèle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 -Représentation du modèle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant notre dataset qui represente les prix des maisons en sachant leurs surface. Pour realiser le modele de regression linéaire, il ne faut trouver (apprendre) une fonction $$ h_\\Theta : X \\rightarrow Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la fonction est appelé h (comme hypothese) par convention.\n",
    "toujours par convention, on met theta pour faire référence aux paramètres de la fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on fait une regression linéaire, la fonction h sera linéaire et est représenté par $$ h_\\Theta (x) = \\Theta_0 + \\Theta_1 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que notre modele soit pertinant, la condition suivante doit etre vrai : pour tout $$ (x^{(i)}, y^{(i)}) \\in E_m $$ on a $$ h_\\Theta (x^{(i)}) \\approx y^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que la condition ci-dessus soit vrai, il nous faut trouvé des valeurs adéquat pour les paramettres (Theta_0 et Theta_1) de la fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, la question qu'on peut se poser est \"comment trouver les bonnes valeurs pour les parametres Theta_0 et Theta_1 ?\".\n",
    "Nous allons y répondre dans la dernière partie de cette article mais avant ca, on va voir la fonction d'erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 - Implémentation vectorielle du modèle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rendre notre fonction h plus performant en temps de calcule on peut le vectoriser. Selon wikipedia, \"La vectorisation est le processus de conversion d'un programme informatique à partir d'une implémentation scalaire, qui traite une seule paire d'opérandes à la fois, à une implémentation vectorielle qui traite une opération sur plusieurs paires d'opérandes à la fois.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour implémenter véctoriellement notre modèle, on doit définir la fonction h_theta de manière vectorielle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour definir la fonction h_theta de maniere vectorielle : \n",
    "- On transforme d'abord les parametres theta en vecteur $$ \\Theta =\\binom{\\Theta_0}{\\Theta_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensuite, on transforme x en vecteur $$ x =\\binom{x_0}{x_1} \\hspace{0.3cm} avec \\hspace{0.3cm} x_0=1 \\hspace{0.3cm} et \\hspace{0.3cm} x_1=x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la fonction h_theta devient $$ h_\\Theta (x) = \\Theta^T x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ avec \\hspace{0.2cm}\\Theta^T = vecteur \\hspace{0.2cm}transposé \\hspace{0.2cm} de \\hspace{0.2cm} \\Theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le reste de l'article et de la série, on implémentera toujours les modèles de manière vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pour \\hspace{0.2cm} tous \\hspace{0.2cm} les \\hspace{0.2cm} x^{(i)} \\in X \\hspace{0.2cm} on \\hspace{0.2cm} aura \\hspace{0.2cm} x^{(i)} =\\binom{x^{(i)}_0}{x^{(i)}_1} \\hspace{0.2cm} avec \\hspace{0.2cm} x^{(i)}_0=1 \\hspace{0.2cm} et \\hspace{0.2cm} x^{(i)}_1=x_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble X des surfaces des maisons va devenir : $$ X = \\begin{bmatrix}\n",
    "1 & x^{(1)}_1\\\\ \n",
    "1 & x^{(2)}_1\\\\ \n",
    " .&. \\\\ \n",
    " .&. \\\\ \n",
    " .&. \\\\ \n",
    "1 & x^{(n)}_1\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on modifie la varable dataset afin d'ajouter une colonne (avec des 1) devant. ce qui va nous donner : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 2)\n",
      "[[1.000e+00 8.450e+03]\n",
      " [1.000e+00 9.600e+03]\n",
      " [1.000e+00 1.125e+04]\n",
      " ...\n",
      " [1.000e+00 9.042e+03]\n",
      " [1.000e+00 9.717e+03]\n",
      " [1.000e+00 9.937e+03]]\n",
      "(1460,)\n",
      "[208500. 181500. 223500. ... 266500. 142125. 147500.]\n"
     ]
    }
   ],
   "source": [
    "datasetTemp = dataset\n",
    "\n",
    "dataset = np.ones((np.size(X),3))\n",
    "dataset[:,1] = datasetTemp[:,0]\n",
    "dataset[:,2] = datasetTemp[:,1]\n",
    "\n",
    "X = dataset[:,0:2]\n",
    "Y = dataset[:,2]\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi implémenter la fonction python qui correspond à h_theta. Cette fonction prend en parametre deux vecteurs x (ou une matrice mx2) et theta et retourne la valeur de h_theta de x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_theta(X, theta):\n",
    "    ret = np.dot(theta, X.T)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allons voir ce que donne la fonction h_theta sur des exemples : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour theta = (0.5, 1), et x = (1, 8450) h_theta(x) vaut :  8450.5\n",
      "pour theta = (0.5, 1), et X est le dataset,  h_theta(x) vaut :  [ 8450.5  9600.5 11250.5 ...  9042.5  9717.5  9937.5]\n"
     ]
    }
   ],
   "source": [
    "# pour theta = (0.5, 1)\n",
    "theta = np.array([0.5, 1])\n",
    "\n",
    "x = np.array([1, 8450])\n",
    "print(\"pour theta = (0.5, 1), et x = (1, 8450) h_theta(x) vaut : \", h_theta(x,theta))\n",
    "\n",
    "print(\"pour theta = (0.5, 1), et X est le dataset,  h_theta(x) vaut : \", h_theta(X,theta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que pour le premier exemple (maison), la surface est 8450 et prix est 208500.\n",
    "Quand on se sert de notre modèle h_theta pour estimer le prix de cette maison on trouve 8450.5, ce qui veut dire que notre modèle n'est pas encore au point.\n",
    "Pour qu'il soit au point il faudra l'entrainer avec notre dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Fonction d'erreur : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a vu que notre modèle fait des erreurs dans l'estimation des prix des maisons. en prenant l'exemple de la première maison de notre dataset qui vaut 208500, notre modèle l'estime à 8450.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut definir l'erreur de notre modèle sur la première maison par : $$ (h_\\Theta (x^{(1)}) - y^{(1)})^2 = (8450.5 - 208500)^2 =40019802450,25 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est un grosse erreur car le prix estimé est trés loin du prix réelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi définir la fonction d'erreur comme la moyene des erreurs de notre modèle sur le dataset E_m\n",
    "Ce qui nous donne : $$ J(\\Theta ) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\Theta (x^{(i)}) - y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est aussi appelé erreur quadratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par convension et pour faciliter le calcul du gradiant, on multiplie cette moyène par 1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'entrainement du modèle, c'est cette fonction J_theta qu'on va essayé de minimiser.\n",
    "Ce qui revient à trouver un vecteur de paramèttre theta=(theta_0, theta_1) pour lequel J_theta serait le plus proche de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi implémenter la fonction python qui correspond à la fonction d'erreur J de theta. Cette fonction prend en parametre un vecteur x (ou une matrice mx2), un vecteur theta et un vecteur Y (ou un scalaire y_i) et retourne la valeur de J de theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeErrorFunction(x, y, theta):\n",
    "    h_theta_de_x = h_theta(x,theta)\n",
    "    vec_temp =  (h_theta_de_x - y)**2\n",
    "    ret = np.sum(vec_temp)/(2*(y.size))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut calculer l'erreur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour theta = (0.5, 1), l'erreur est : 17512844306.44418\n",
      "pour theta = (500, 1000), l'erreur est : 50740190897.50479\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([0.5, 1])\n",
    "print(\"pour theta = (0.5, 1), l'erreur est :\", computeErrorFunction(X, Y, theta))\n",
    "\n",
    "theta = np.array([12, 30])\n",
    "print(\"pour theta = (500, 1000), l'erreur est :\", computeErrorFunction(X, Y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que l'erreur est plus petite avec theta = (500, 1000) qu'avec theta = (0.5, 1), donc notre modèle est plus performant avec le parametre theta = (500, 1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Entrainement du Modèle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrainement du modèle consiste à trouver le vecteur paramètre theta = (theta_0, theta_1) pour lequel la fonction d'erreur J(theta) est le plus proche de 0. Pour se faire, on doit minimiser la fonction J(theta) : $$ \\underset{\\Theta_{0}, \\Theta_{1}}{min}J(\\Theta) $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On minimise J(theta) en faisant une descente de gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - la descente de gradient : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descente de gradient est un algorithme qui permet de trouver le minimum d'une focntion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut trouver le minimum d'une fonction f(x) par exemple $$ \\underset{x}{min}f(x) $$, calculer f(x) pour toutes les valeur possibles de x serait trés couteux en ressources informatique et en temps de calcul. Il existe un meilleur outils (largement utilisé en machine learning) qui s'appele la descente de gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- La première etape de la descente de gradient est de choisir au hasard une valeur x_0 de départ pour x.\n",
    "2- Ensuite, on calcule le gradient de la fonction f au point x_0 (qui est f'(x_0)). En effet, le gradient est un vecteur qui indique toujours la direction de la croissance maximale de la fonction. Le schéma suivant va illustrer ca :"
   ]
  },
  {
   "attachments": {
    "desc-grad-1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![desc-grad-1.png](attachment:desc-grad-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Comme le gradient indique la direction de la valeur maximale de la fonction, donc pour avoire une nouvelle valeur de x qui nous permette de nous rapprocher de la valeur minimale de la fonction f(x) on fait un pas vers la direction inverse du gradient. ce qui nous donne une nouvelle valeur x_1 = x_0 - alpha*f'(x_0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- on repéte les etapes 1, 2, et 3 avec la nouvelle valeur de x_1 de x.\n",
    "5- En iterant plusieurs fois sur ce procédé, on va arriver à une valeur x_n tel que $$ f(x_{n}) \\approx 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et la, on dit qu'on a atteint notre point de convergence."
   ]
  },
  {
   "attachments": {
    "desc-grad-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![desc-grad-2.png](attachment:desc-grad-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha est un paramètre de l'algorithme de descente de gradient. On l'appelle le pas d'apprentissage (learning rate en anglais).\n",
    "la valeur de alpha doit etre fixé avant le debut de la descente de grandient. Il ne doit ni trop petit ni trop grand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Application de la descente de gradient à notre modèle de regression linéaire : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrainement du modèle consiste à minimiser la fonction d'erreur : $$ \\underset{\\Theta_{0}, \\Theta_{1}}{min}J(\\Theta) $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se faire, nous allons appliquer le descente de gradient sur la fonction J(theta).\n",
    "Dans le cas de notre fonction J(theta), on a deux inconnues theta_0 et theta_1. au lieu de calculer J'(theta), on calcule les dérivés partiels $$ \\frac{\\partial}{\\partial \\Theta_0}J(\\Theta) \\hspace{0.2cm} et \\hspace{0.2cm} \\frac{\\partial}{\\partial \\Theta_1}J(\\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui sont  : $$ \\frac{\\partial}{\\partial \\Theta_0}J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\Theta_1}J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}(\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabord, on choisi des valeurs initiaux (au hasarrd) pour theta_0 et theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on fait des itérations : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on a : repeter { $$ \\Theta_0 := \\Theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Theta_1 := \\Theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En machine learning, chaque itération est appelé epoque. le nombre d'époque est un paramèttre de l'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour implémenter véctoriellement l'entrainement de notre modèle, On transforme les parametres theta et les gradients (dérivés partielles) en vecteur $$ \\Theta =\\binom{\\Theta_0}{\\Theta_1} \\hspace{0.2cm} et \\hspace{0.2cm} \\triangledown J(\\Theta ) = \\binom{\\frac{\\partial}{\\partial \\Theta_0}J(\\Theta)}{\\frac{\\partial}{\\partial \\Theta_1}J(\\Theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre itération devient :  repeter { $$ \\Theta := \\Theta - \\alpha \\hspace{0.1cm} \\triangledown J(\\Theta ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi implémenter la fonction python qui calcule les gradients. Cette fonction prend en parametre un vecteur une matrice mx2, un vecteur theta et un vecteur Y et retourne le vercteur de gradient $$ \\triangledown J(\\Theta ) $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(x, y, theta):\n",
    "    h_theta_de_x = h_theta(x,theta)\n",
    "    vec_temp =  (h_theta_de_x - y)\n",
    "    ret = np.dot(x.T,vec_temp)/y.size\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut calculer le vecteur des gradients pour voir ce que ca donne : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour theta = (0.5, 1), le vecteur des gradients est : [-1.70403868e+05 -1.90161854e+09]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([0.5, 1])\n",
    "print(\"pour theta = (0.5, 1), le vecteur des gradients est :\", computeGradient(X, Y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi implémenter la fonction python qui entraine notre modèle. Cette fonction prend en parametre un vecteur une matrice mx2, un vecteur Y, le nombre d'epoques, et le learning rate (alpha) et retourne le vercteur de theta pour lequel notre fonction d'erreur sera minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='warn')\n",
    "np.set_printoptions(suppress=True)\n",
    "def trainModel(x, y, nb_epoch, alpha=2):\n",
    "    # on choisi des valeurs arbitraires pour le vecteur theta\n",
    "    theta = np.array([12, 23])\n",
    "    # on itere sur le nombre d'epoques\n",
    "    for i in range(nb_epoch):\n",
    "        grad = computeGradient(x, y, theta)\n",
    "        theta = theta - alpha*grad\n",
    "        if i == 15 : \n",
    "            print(\"theta \", theta)\n",
    "            print(\"gradient :\", grad)\n",
    "            print(\"h_theta : \",h_theta(x,theta))\n",
    "            \n",
    "        print(\"Epoque \", i)\n",
    "        print(\"l'erreur est : \", \"%.5f\"%computeErrorFunction(x, y, theta))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainons notre modèle sur 10 epoques par exemple. ce qui nous donne : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque  0\n",
      "l'erreur est :  77859732537951528157184.00000\n",
      "Epoque  1\n",
      "l'erreur est :  343888065122202927648190320640786432.00000\n",
      "Epoque  2\n",
      "l'erreur est :  1518872432240406566428139043658078160294028247040.00000\n",
      "Epoque  3\n",
      "l'erreur est :  6708501106603084323558202062909336433289529595405010003296256.00000\n",
      "Epoque  4\n",
      "l'erreur est :  29629866302143559587893603476521753851066034041860167850389637231781347328.00000\n",
      "Epoque  5\n",
      "l'erreur est :  130868127340513065674569562113885557634874213695174987025951128111802410475734526066688.00000\n",
      "Epoque  6\n",
      "l'erreur est :  578013635936443265560346563544150374485911653597800435277730327189754070139133924585913609426567168.00000\n",
      "Epoque  7\n",
      "l'erreur est :  2552949829098984081329956788172309184330548339606438274329654063953826784353322220821357179550141143221706686464.00000\n",
      "Epoque  8\n",
      "l'erreur est :  11275776944842176881676354703397064043200317079685027609575249721275915127589633727312551946941348949560568996329518796898304.00000\n",
      "Epoque  9\n",
      "l'erreur est :  49802445884612946253021354305641364515358563886855868737067928253864788451102911879113814054640388593295182698746194339713876063714017280.00000\n",
      "Epoque  10\n",
      "l'erreur est :  219965650989960775566177908609812713813977976860040303241991060760986367964690372295763902122172487761354674034810762776584569713722823101717864775680.00000\n",
      "Epoque  11\n",
      "l'erreur est :  971536372481382543256374192574140502152094703013759212574259146081570954682648513884223461322384861521661366101191707397601812885337494869382307652891037971513344.00000\n",
      "Epoque  12\n",
      "l'erreur est :  4291046892123010893268243448540354795222017071259172458130209707865203281713794786706809124691104622230805568532364928616958014478881592232036048921629076589334729936393469952.00000\n",
      "Epoque  13\n",
      "l'erreur est :  18952541512542628289187164763760973387435007716914716520568694981793979226717961301265415326349693793632921497072604930542540330621627988443056360617156245148911130538294471714881799192576.00000\n",
      "Epoque  14\n",
      "l'erreur est :  83708903401644586504321846929610850464247266642312714268210240802115665633329669513380534759613587939448338410662447804279554617536707092337566245755577478069992566264826296565801343034714648987303936.00000\n",
      "theta  [9.38662010e+097 1.87575784e+102]\n",
      "gradient : [-9.38662457e+099 -1.87575873e+104]\n",
      "h_theta :  [1.58501538e+106 1.80072753e+106 2.11022757e+106 ... 1.69606024e+106\n",
      " 1.82267390e+106 1.86394057e+106]\n",
      "Epoque  15\n",
      "l'erreur est :  369722472527951037809850879604142154937064829988417696366612581048948203284724521481239872753735666247790069107053556841651960458714822652865972225122149104280534726755267191267469672133134991580165806968287526912.00000\n",
      "Epoque  16\n",
      "l'erreur est :  1632976913295643013476748186292594898658577859313291313454046280342322531544312139780941217427501983374316480984521970494110489395116298527619210364033467044988089119905832997350535628335609647869490447095982404881880547590144.00000\n",
      "Epoque  17\n",
      "l'erreur est :  7212473672816764927421864363743898300435786296065040549678127100110137839937577693296765148322996428332639875322512271009044179786724082683845437389168388719447352803134299774371758080928816303380404060785023137500735043725467170885337088.00000\n",
      "Epoque  18\n",
      "l'erreur est :  31855794198639201364480537813805741275475510153918401672211015578270225743087228036882048536279272469578821774446984056103710433988084343749106783822097642747126573837434965352238875864633893290866147239441068643069983443248611682585937484951345692672.00000\n",
      "Epoque  19\n",
      "l'erreur est :  140699525580345684579010575254880085751621567980091860923530753117327961293110432370721095283437210948960385985491528958344187155596710541798137086927653498985245048627613410298943526460080606221886540088649054268964700175977996953927730876618160400157377283751936.00000\n",
      "Epoque  20\n",
      "l'erreur est :  621436601928449867745676483372005563056206284426487299294880263138618236587281241706034739258894355458283818134751084991911277592860873276818665888624573611895725331608559975127630454887807380624627439543445335227567282452457349162231914472674638734505348119188928530124636160.00000\n",
      "Epoque  21\n",
      "l'erreur est :  2744738822845934622299221890786445167389518867414950210351241803040851579712887200290548565647669022153194339190702532096405152877194896310919206157078393878768776115808712967997116710839594333314824078660911125299484743983613765571237770756763513449082284681493521399252555062046487478272.00000\n",
      "Epoque  22\n",
      "l'erreur est :  12122863671465956341445160392283037471162497286252839935187696978550276517365712869898382060194148979610905424662640845215428308087405434111627807164959249320877161405486377419319767575082684568971058634361492259399792763496007817260991115879682591396626419457072039360252889925352724479322890419830784.00000\n",
      "Epoque  23\n",
      "l'erreur est :  inf\n",
      "Epoque  24\n",
      "l'erreur est :  inf\n",
      "Epoque  25\n",
      "l'erreur est :  inf\n",
      "Epoque  26\n",
      "l'erreur est :  inf\n",
      "Epoque  27\n",
      "l'erreur est :  inf\n",
      "Epoque  28\n",
      "l'erreur est :  inf\n",
      "Epoque  29\n",
      "l'erreur est :  inf\n",
      "Epoque  30\n",
      "l'erreur est :  inf\n",
      "Epoque  31\n",
      "l'erreur est :  inf\n",
      "Epoque  32\n",
      "l'erreur est :  inf\n",
      "Epoque  33\n",
      "l'erreur est :  inf\n",
      "Epoque  34\n",
      "l'erreur est :  inf\n",
      "Epoque  35\n",
      "l'erreur est :  inf\n",
      "Epoque  36\n",
      "l'erreur est :  inf\n",
      "Epoque  37\n",
      "l'erreur est :  inf\n",
      "Epoque  38\n",
      "l'erreur est :  inf\n",
      "Epoque  39\n",
      "l'erreur est :  inf\n",
      "Epoque  40\n",
      "l'erreur est :  inf\n",
      "Epoque  41\n",
      "l'erreur est :  inf\n",
      "Epoque  42\n",
      "l'erreur est :  inf\n",
      "Epoque  43\n",
      "l'erreur est :  inf\n",
      "Epoque  44\n",
      "l'erreur est :  inf\n",
      "Epoque  45\n",
      "l'erreur est :  inf\n",
      "Epoque  46\n",
      "l'erreur est :  inf\n",
      "Epoque  47\n",
      "l'erreur est :  inf\n",
      "Epoque  48\n",
      "l'erreur est :  nan\n",
      "Epoque  49\n",
      "l'erreur est :  nan\n",
      "Epoque  50\n",
      "l'erreur est :  nan\n",
      "Epoque  51\n",
      "l'erreur est :  nan\n",
      "Epoque  52\n",
      "l'erreur est :  nan\n",
      "Epoque  53\n",
      "l'erreur est :  nan\n",
      "Epoque  54\n",
      "l'erreur est :  nan\n",
      "Epoque  55\n",
      "l'erreur est :  nan\n",
      "Epoque  56\n",
      "l'erreur est :  nan\n",
      "Epoque  57\n",
      "l'erreur est :  nan\n",
      "Epoque  58\n",
      "l'erreur est :  nan\n",
      "Epoque  59\n",
      "l'erreur est :  nan\n",
      "Epoque  60\n",
      "l'erreur est :  nan\n",
      "Epoque  61\n",
      "l'erreur est :  nan\n",
      "Epoque  62\n",
      "l'erreur est :  nan\n",
      "Epoque  63\n",
      "l'erreur est :  nan\n",
      "Epoque  64\n",
      "l'erreur est :  nan\n",
      "Epoque  65\n",
      "l'erreur est :  nan\n",
      "Epoque  66\n",
      "l'erreur est :  nan\n",
      "Epoque  67\n",
      "l'erreur est :  nan\n",
      "Epoque  68\n",
      "l'erreur est :  nan\n",
      "Epoque  69\n",
      "l'erreur est :  nan\n",
      "Epoque  70\n",
      "l'erreur est :  nan\n",
      "Epoque  71\n",
      "l'erreur est :  nan\n",
      "Epoque  72\n",
      "l'erreur est :  nan\n",
      "Epoque  73\n",
      "l'erreur est :  nan\n",
      "Epoque  74\n",
      "l'erreur est :  nan\n",
      "Epoque  75\n",
      "l'erreur est :  nan\n",
      "Epoque  76\n",
      "l'erreur est :  nan\n",
      "Epoque  77\n",
      "l'erreur est :  nan\n",
      "Epoque  78\n",
      "l'erreur est :  nan\n",
      "Epoque  79\n",
      "l'erreur est :  nan\n",
      "Epoque  80\n",
      "l'erreur est :  nan\n",
      "Epoque  81\n",
      "l'erreur est :  nan\n",
      "Epoque  82\n",
      "l'erreur est :  nan\n",
      "Epoque  83\n",
      "l'erreur est :  nan\n",
      "Epoque  84\n",
      "l'erreur est :  nan\n",
      "Epoque  85\n",
      "l'erreur est :  nan\n",
      "Epoque  86\n",
      "l'erreur est :  nan\n",
      "Epoque  87\n",
      "l'erreur est :  nan\n",
      "Epoque  88\n",
      "l'erreur est :  nan\n",
      "Epoque  89\n",
      "l'erreur est :  nan\n",
      "Epoque  90\n",
      "l'erreur est :  nan\n",
      "Epoque  91\n",
      "l'erreur est :  nan\n",
      "Epoque  92\n",
      "l'erreur est :  nan\n",
      "Epoque  93\n",
      "l'erreur est :  nan\n",
      "Epoque  94\n",
      "l'erreur est :  nan\n",
      "Epoque  95\n",
      "l'erreur est :  nan\n",
      "Epoque  96\n",
      "l'erreur est :  nan\n",
      "Epoque  97\n",
      "l'erreur est :  nan\n",
      "Epoque  98\n",
      "l'erreur est :  nan\n",
      "Epoque  99\n",
      "l'erreur est :  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a605199\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in multiply\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\a605199\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in subtract\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_train = trainModel(X, Y, 100, 0.01)\n",
    "x = np.array([1, 8450])\n",
    "h_theta(x,theta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "a=np.array([  0.,   1.,   4.])\n",
    "b=np.array([[1,2,3],[4,5,6]])\n",
    "print(np.sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### passage de l'implementation scalaire à l'implementation vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut demontrer $$ h_\\Theta (x) = \\Theta_0 + \\Theta_1 x = \\Theta_0 * 1 + \\Theta_1 * x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ comme \\hspace{0.2cm} x_0 = 1 \\hspace{0.2cm} et \\hspace{0.2cm} x_1 = x, \\hspace{0.3cm} on \\hspace{0.3cm} peut \\hspace{0.3cm} ecrire \\hspace{0.3cm} h_\\Theta (x) = \\Theta_0 * x_0 + \\Theta_1 * x_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ce \\hspace{0.2cm} qui \\hspace{0.2cm} nous \\hspace{0.2cm} donne \\hspace{0.2cm} h_\\Theta (x) = \\bigl(\\begin{smallmatrix}\n",
    " \\Theta_0& \\Theta_1\n",
    "\\end{smallmatrix}\\bigr) * \\binom{x_0}{x_1} = \\Theta^Tx $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_\\Theta (x) = \\Theta^T x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
