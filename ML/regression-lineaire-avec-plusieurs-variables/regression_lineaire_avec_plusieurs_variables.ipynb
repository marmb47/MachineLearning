{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression-lineaire-avec-plusieurs-variables.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1eSp0m08qPF"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tTbk2pD8gFw"
      },
      "source": [
        "Cet article est le troisième de la série intitulée \"Machine Learning pour débutant\". Dans ce tutoriel, nous allons parler de la régression linéaire avec plusieures variables plus connu sous le nom de \"Multivariate linear regression\" . Pour ce faire, aprés avoir explorer les données, nous allons représenter le modèle et la fonction d'erreur avant d'entrainer le modèle et de trouver l'équation normale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnwh0EX98xO5"
      },
      "source": [
        "# 1- Exploration des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFnNLfk9JgO"
      },
      "source": [
        "Dans cet article, Nous allons réaliser un modèle qui permet de prédire le prix d’une maison à partir de sa surface en metre carré et du nombre de chambres. Pour ce faire, nous disposons d’un dataset qui contient les informations (surface, nombre de chambres, et prix) de plusieurs maison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh1V7QyIPLRd"
      },
      "source": [
        "Le dataset a été téléchargé depuis Coursera. Il s’agit d’un fichier csv(houses-pricing.txt) qui contient trois champs qui portent sur les  différentes maisons. Chaque ligne représente une maison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqFgQOBAVL_U"
      },
      "source": [
        "Les trois champs sont :\n",
        "\n",
        "*   Le premier champ représente la surface de la maison en metre carré.\n",
        "*   Le deuxième champ représente le nombre de chambres de la maison.\n",
        "*   Le troisième champ représente le prix de la maison en (1000$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ztVdCFWVk2i"
      },
      "source": [
        "Maintenant, recupérons le dataset pour voir à quoi ils ressemblent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoYbJPXVV0M7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "0da85b38-eac4-43b1-cc2b-c0070801971b"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "import numpy as np\n",
        "tab = np.genfromtxt('houses-pricing.txt', delimiter=',')\n",
        "\n",
        "# recuperation des surfaces des maisons\n",
        "x1 = tab[:,0];\n",
        " \n",
        "# recuperation des nombres de chambres pour chaque maisons\n",
        "x2 = tab[:,1];\n",
        "\n",
        "# recuperation des prix des maisons\n",
        "Y = tab[:,2];\n",
        "\n",
        " \n",
        "dataset = np.ones((np.size(x1),3))\n",
        "dataset[:,0] = x1\n",
        "dataset[:,1] = x2\n",
        "dataset[:,2] = Y\n",
        " \n",
        "# on affiche les 10 premiers maisons\n",
        "print(dataset[0:9,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-800b7bdf-2405-480e-9f0e-357f69f01aa1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-800b7bdf-2405-480e-9f0e-357f69f01aa1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving houses-pricing.txt to houses-pricing.txt\n",
            "[[2.10400e+03 3.00000e+00 3.99900e+05]\n",
            " [1.60000e+03 3.00000e+00 3.29900e+05]\n",
            " [2.40000e+03 3.00000e+00 3.69000e+05]\n",
            " [1.41600e+03 2.00000e+00 2.32000e+05]\n",
            " [3.00000e+03 4.00000e+00 5.39900e+05]\n",
            " [1.98500e+03 4.00000e+00 2.99900e+05]\n",
            " [1.53400e+03 3.00000e+00 3.14900e+05]\n",
            " [1.42700e+03 3.00000e+00 1.98999e+05]\n",
            " [1.38000e+03 3.00000e+00 2.12000e+05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVjaVmcgYlGg"
      },
      "source": [
        "On a recupéré les surfaces des maisons (x1), les nombres de chambres pour chaque maisons (x2) et les prix des maisons correspondantes (Y) qu’on a mis sur un tableau à trois colonnes qui se nomme “dataset”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWimV3FIZn7K"
      },
      "source": [
        "On nomme m le nombre d’exemples (de maisons) dans le dataset. On obtient ainsi l’ensemble $$E_m = \\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\} $$\n",
        "\n",
        "$$avec \\: \\: \\: \\: x^{(i)}=\\begin{pmatrix}\n",
        "x^{(i)}_{1}\\\\ \n",
        "x^{(i)}_{2}\n",
        "\\end{pmatrix} \\: \\forall \\: 0 < i \\leq m $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjndpaf_c_Zw"
      },
      "source": [
        "De ce fait $$ (x^{(i)},y^{(i)}) $$ représente la i-ème ligne de la table “dataset” :  \n",
        "* la surface $$ x^{(i)}_1 $$ de la i-ème maison\n",
        "* le nombre de chambres $$ x^{(i)}_2 $$ de la i-ème maison \n",
        "* et le prix $$ y^{(i)} $$ de la i-ème maison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QJyzBLcbdo"
      },
      "source": [
        "Ici, X est un vecteur de dimension 2 contrairement à l'article précédent ou X était un sacalaire. ce qui veut dire qu'on fait une régréssion avec 2 variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeltGo_ufOib"
      },
      "source": [
        "Si on représente vectoriellement tout le dataset, on aura :\n",
        "\n",
        "$$ \\forall \\: x^{(i)} \\in \\: X, \\: on \\: aura \\: \\: x^{(i)}=\\begin{pmatrix}\n",
        "x^{(i)}_{0}\\\\ \n",
        "x^{(i)}_{1}\\\\ \n",
        "x^{(i)}_{2}\n",
        "\\end{pmatrix} \\: avec \\: x^{(i)}_{0} = 1 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUjDWBxnhOik"
      },
      "source": [
        "L’ensemble X va devenir alors : $$ X = \\begin{pmatrix}\n",
        "1 & x^{(1)}_{1} & x^{(1)}_{2}\\\\ \n",
        "1 & x^{(2)}_{1} & x^{(2)}_{2}\\\\ \n",
        ". & . & .\\\\\n",
        ". & . & .\\\\\n",
        "1 & x^{(m)}_{1} & x^{(m)}_{2}\n",
        "\\end{pmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXv2xFmFiYTz"
      },
      "source": [
        "on modifie la varable dataset afin d’ajouter une colonne (avec des 1) devant. ce qui va nous donner :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUr2ySMRiaiz",
        "outputId": "dac8c435-d6a1-4d3c-a57e-e8921484d4aa"
      },
      "source": [
        "datasetTemp = dataset\n",
        " \n",
        "dataset = np.ones((np.size(x1),4))\n",
        "dataset[:,1] = datasetTemp[:,0]\n",
        "dataset[:,2] = datasetTemp[:,1]\n",
        "dataset[:,3] = datasetTemp[:,2]\n",
        " \n",
        "X = dataset[:,0:3]\n",
        "Y = dataset[:,3]\n",
        " \n",
        "print(X.shape)\n",
        "print(X)\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47, 3)\n",
            "[[1.000e+00 2.104e+03 3.000e+00]\n",
            " [1.000e+00 1.600e+03 3.000e+00]\n",
            " [1.000e+00 2.400e+03 3.000e+00]\n",
            " [1.000e+00 1.416e+03 2.000e+00]\n",
            " [1.000e+00 3.000e+03 4.000e+00]\n",
            " [1.000e+00 1.985e+03 4.000e+00]\n",
            " [1.000e+00 1.534e+03 3.000e+00]\n",
            " [1.000e+00 1.427e+03 3.000e+00]\n",
            " [1.000e+00 1.380e+03 3.000e+00]\n",
            " [1.000e+00 1.494e+03 3.000e+00]\n",
            " [1.000e+00 1.940e+03 4.000e+00]\n",
            " [1.000e+00 2.000e+03 3.000e+00]\n",
            " [1.000e+00 1.890e+03 3.000e+00]\n",
            " [1.000e+00 4.478e+03 5.000e+00]\n",
            " [1.000e+00 1.268e+03 3.000e+00]\n",
            " [1.000e+00 2.300e+03 4.000e+00]\n",
            " [1.000e+00 1.320e+03 2.000e+00]\n",
            " [1.000e+00 1.236e+03 3.000e+00]\n",
            " [1.000e+00 2.609e+03 4.000e+00]\n",
            " [1.000e+00 3.031e+03 4.000e+00]\n",
            " [1.000e+00 1.767e+03 3.000e+00]\n",
            " [1.000e+00 1.888e+03 2.000e+00]\n",
            " [1.000e+00 1.604e+03 3.000e+00]\n",
            " [1.000e+00 1.962e+03 4.000e+00]\n",
            " [1.000e+00 3.890e+03 3.000e+00]\n",
            " [1.000e+00 1.100e+03 3.000e+00]\n",
            " [1.000e+00 1.458e+03 3.000e+00]\n",
            " [1.000e+00 2.526e+03 3.000e+00]\n",
            " [1.000e+00 2.200e+03 3.000e+00]\n",
            " [1.000e+00 2.637e+03 3.000e+00]\n",
            " [1.000e+00 1.839e+03 2.000e+00]\n",
            " [1.000e+00 1.000e+03 1.000e+00]\n",
            " [1.000e+00 2.040e+03 4.000e+00]\n",
            " [1.000e+00 3.137e+03 3.000e+00]\n",
            " [1.000e+00 1.811e+03 4.000e+00]\n",
            " [1.000e+00 1.437e+03 3.000e+00]\n",
            " [1.000e+00 1.239e+03 3.000e+00]\n",
            " [1.000e+00 2.132e+03 4.000e+00]\n",
            " [1.000e+00 4.215e+03 4.000e+00]\n",
            " [1.000e+00 2.162e+03 4.000e+00]\n",
            " [1.000e+00 1.664e+03 2.000e+00]\n",
            " [1.000e+00 2.238e+03 3.000e+00]\n",
            " [1.000e+00 2.567e+03 4.000e+00]\n",
            " [1.000e+00 1.200e+03 3.000e+00]\n",
            " [1.000e+00 8.520e+02 2.000e+00]\n",
            " [1.000e+00 1.852e+03 4.000e+00]\n",
            " [1.000e+00 1.203e+03 3.000e+00]]\n",
            "(47,)\n",
            "[399900. 329900. 369000. 232000. 539900. 299900. 314900. 198999. 212000.\n",
            " 242500. 239999. 347000. 329999. 699900. 259900. 449900. 299900. 199900.\n",
            " 499998. 599000. 252900. 255000. 242900. 259900. 573900. 249900. 464500.\n",
            " 469000. 475000. 299900. 349900. 169900. 314900. 579900. 285900. 249900.\n",
            " 229900. 345000. 549000. 287000. 368500. 329900. 314000. 299000. 179900.\n",
            " 299900. 239500.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRGpLQa-2G3l",
        "outputId": "a79ec745-a13d-489e-b13d-f0176b49059d"
      },
      "source": [
        "print(np.min(X, axis=0))\n",
        "print(np.max(X, axis=0))\n",
        "print(np.mean(X, axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1. 852.   1.]\n",
            "[1.000e+00 4.478e+03 5.000e+00]\n",
            "[1.00000000e+00 2.00068085e+03 3.17021277e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHR1CPVc9K8G"
      },
      "source": [
        "# 2 - Représentation du modèle et fonction d'erreur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhniJWZH1tDU"
      },
      "source": [
        "## 2.1 - Représentation du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMBoI0Q-9l3G"
      },
      "source": [
        "Aprés avoir explorer notre dataset qui représente les prix de différentes maisons sachant leurs surfaces (en mètre carré) et leurs nombres de chambres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOhwIHw7vguI"
      },
      "source": [
        "Pour realiser le modele de régression linéaire, il nous faut trouver (apprendre) une fonction $$ h_{\\Theta} : X \\rightarrow Y $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_NQEfXpk6gl"
      },
      "source": [
        "d'une manière plus générale, la fonction hypothèse d'une régression lineaire avec plusieurs variables (Multivariate regression) est : $$ h_{\\Theta} (x^{(i)}) = \\Theta _{0} + \\Theta _{1}x^{(i)}_{1} + \\Theta _{2}x^{(i)}_{2} + ... + \\Theta _{n}x^{(i)}_{n} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLA7lybfwDIo"
      },
      "source": [
        "Dans notre cas (modèle qui permet de prédire le prix d’une maison à partir de sa surface en metre carré et du nombre de chambres), la fonction h sera linéaire et représenté par : $$ h_{\\Theta} (x^{(i)}) = \\Theta _{0} + \\Theta _{1}x^{(i)}_{1} + \\Theta _{2}x^{(i)}_{2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChqVROZzxaYG"
      },
      "source": [
        "L'écriture vectorielle de la fonction h sera : $$ h_{\\Theta} (x^{(i)}) = \\Theta^{T} x^{(i)}  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coeVTnmGy4rC"
      },
      "source": [
        "$$ avec \\, \\, \\Theta = \\begin{pmatrix}\n",
        "\\Theta _{0}\\\\ \n",
        "\\Theta _{1}\\\\ \n",
        "\\Theta _{2}\n",
        "\\end{pmatrix}\n",
        "et \\, \\, x^{(i)} = \\begin{pmatrix}\n",
        "x^{(i)}_{0}\\\\ \n",
        "x^{(i)}_{1}\\\\ \n",
        "x^{(i)}_{2}\n",
        "\\end{pmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMNkwRZkzHA0"
      },
      "source": [
        "$$ x^{(i)} \\, \\, represente \\, une \\, maison \\, avec \\\\  x^{(i)}_{0} = 1;\\\\\n",
        "x^{(i)}_{1} = surface \\, de \\, la \\, maison \\, (en \\, m^{2});\\\\\n",
        "x^{(i)}_{2} = nombre \\, de \\, chambres \\, de \\, la \\, maison. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmgs0A5v7VoE"
      },
      "source": [
        "On peut ainsi implémenter la fonction python qui correspond à hΘ. Cette fonction prend en paramètres deux vecteurs :\n",
        "\n",
        "- x (ou une matrice mx3).\n",
        "- Θ correspondant aux paramètres de notre modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMYmOydd7fjd"
      },
      "source": [
        "On définit la fonction python qui représente notre modèle de la manière suivante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtjaurY-7miy"
      },
      "source": [
        "def h_theta(X, theta):\n",
        "    ret = np.dot(theta, X.T)\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dk3cnQb8NFm"
      },
      "source": [
        "voyons ce que donne la fonction h_theta sur notre dataset : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bUhfvna8YEj",
        "outputId": "15632ff2-00ee-418e-a0af-f7981deb5788"
      },
      "source": [
        "# pour theta = (0.5, 1, 2)\n",
        "theta = np.array([0.5, 1, 2])\n",
        " \n",
        "x = np.array([1, 2104, 3])\n",
        "print(\"pour theta = (0.5, 1, 2), et x = (1, 2104, 3) h_theta(x) vaut : \", h_theta(x,theta))\n",
        " \n",
        "print(\"pour theta = (0.5, 1, 2), et X est le dataset,  h_theta(x) vaut : \", h_theta(X,theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pour theta = (0.5, 1, 2), et x = (1, 2104, 3) h_theta(x) vaut :  2110.5\n",
            "pour theta = (0.5, 1, 2), et X est le dataset,  h_theta(x) vaut :  [2110.5 1606.5 2406.5 1420.5 3008.5 1993.5 1540.5 1433.5 1386.5 1500.5\n",
            " 1948.5 2006.5 1896.5 4488.5 1274.5 2308.5 1324.5 1242.5 2617.5 3039.5\n",
            " 1773.5 1892.5 1610.5 1970.5 3896.5 1106.5 1464.5 2532.5 2206.5 2643.5\n",
            " 1843.5 1002.5 2048.5 3143.5 1819.5 1443.5 1245.5 2140.5 4223.5 2170.5\n",
            " 1668.5 2244.5 2575.5 1206.5  856.5 1860.5 1209.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihnxgrj9haL"
      },
      "source": [
        "On voit que pour le premier exemple (maison), la surface de la maison est 2104, le nombre de cahmbres est 3, et le prix 399900. Si on utilise notre modèle h_theta pour estimer le prix de cette maison on trouve 2110.5, ce qui veut dire que notre modèle n’est pas encore performant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz1gAxIK11Pu"
      },
      "source": [
        "## 2.2 - Représentation de la fonction d'erreur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mrJaUPh_BrW"
      },
      "source": [
        "Nous avons vu que notre modèle fait des erreurs dans l’estimation des prix des maisons. Nous pouvons ainsi représenter l'erreur quadratique de notre modèle par la fonction suivante : $$ J(\\Theta ) = \\frac{1}{2m}\\sum ^{m}_{i=1}(h_{\\Theta }(x^{(i)}) - y^{(i)})^{2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2C1BPREA3HN"
      },
      "source": [
        "On peut ainsi implémenter la fonction python qui correspond à la fonction d’erreur J(Θ). Cette fonction prend en paramètres deux vecteurs :\n",
        "\n",
        "- x (ou une matrice mx3).\n",
        "- Θ correspondant aux paramètres de notre modèle.\n",
        "- Un vecteur Y (ou un scalaire yi)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yrs3ZHbBXk8"
      },
      "source": [
        "def computeErrorFunction(x, y, theta):\n",
        "    h_theta_de_x = h_theta(x,theta)\n",
        "    #print(h_theta_de_x.shape)\n",
        "    #print(y.shape)\n",
        "    vec_temp =  (h_theta_de_x - y)**2\n",
        "    #print(vec_temp.shape)\n",
        "    ret = np.sum(vec_temp)/(2*(y.size))\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gEeK0OaByJ3"
      },
      "source": [
        "On peut calculer l’erreur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYziYR5AB1kB",
        "outputId": "20000b97-11dd-4d70-e132-0e12c3b07176"
      },
      "source": [
        "theta = np.array([0.5, 1, 2])\n",
        "print(\"pour theta = (0.5, 1, 2), l'erreur de notre modèle sur notre dataset est :\", computeErrorFunction(X, Y, theta))\n",
        " \n",
        "theta = np.array([1200, 30000, 45000])\n",
        "print(\"pour theta = (1200, 300, 450), l'erreur de notre modèle sur notre dataset est :\", computeErrorFunction(X, Y, theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pour theta = (0.5, 1, 2), l'erreur de notre modèle sur notre dataset est : 64827252828.65691\n",
            "pour theta = (1200, 300, 450), l'erreur de notre modèle sur notre dataset est : 2065558659029510.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqb5iosF9pfV"
      },
      "source": [
        "# 3 - Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daRntpPC99Re"
      },
      "source": [
        "## 3.1 - Descente de gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qyySaWVp_nO"
      },
      "source": [
        "si nous avons un modèle de regression de n variables alors nous a n+1 paramètres theta. l'entrainement du modèle consistera à minimiser la fonction d'erreur $$ \\underset{\\Theta_{0}, \\Theta_{1},...,\\Theta_{n+1}}{min}J(\\Theta) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlpJ1Tpm-Grm"
      },
      "source": [
        "ce qui reviendrait à faire le traitement suivant : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIMl2PFsAUm"
      },
      "source": [
        "repeter {$$ \n",
        "\\Theta_0 := \\Theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right ) \\\\ \n",
        "\\Theta_1 := \\Theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_1 \\\\ \n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "\\Theta_n := \\Theta_n - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_n \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4x8TezWtN-8"
      },
      "source": [
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU1BGwm7tgWL"
      },
      "source": [
        "Dans notre cas (modèle qui permet de prédire le prix d’une maison à partir de sa surface en metre carré et du nombre de chambres), le traitement sera  :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5p5XvSJtrgs"
      },
      "source": [
        "repeter {$$ \n",
        "\\Theta_0 := \\Theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right ) \\\\ \n",
        "\\Theta_1 := \\Theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_1 \\\\ \n",
        "\\Theta_2 := \\Theta_2 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_2 \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E03hlOauesT"
      },
      "source": [
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWjv5sgfvjUj"
      },
      "source": [
        "Sachant que les dérivés partielles par rapport au différents paramètres sont les suivants : $$ \\frac{\\partial}{\\partial \\Theta_0}J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )  \\\\\n",
        "\\frac{\\partial}{\\partial \\Theta_1}J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_1 \\\\\n",
        "\\frac{\\partial}{\\partial \\Theta_2}J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left ( h_\\Theta (x^{(i)}) - y^{(i)} \\right )x^{(i)}_2 \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zvi6fC9x927"
      },
      "source": [
        "Nous pouvons réécrire vectoriellement nos paramètres et nos gradients par  : $$ \\Theta =\\begin{pmatrix}\n",
        "\\Theta _{0}\\\\ \n",
        "\\Theta _{1}\\\\ \n",
        "\\Theta _{2}\n",
        "\\end{pmatrix} \\hspace{0.2cm} et \\hspace{0.2cm} \\triangledown J(\\Theta ) = \\begin{pmatrix}\n",
        "{\\frac{\\partial}{\\partial \\Theta_0}J(\\Theta)} \\\\ \n",
        "{\\frac{\\partial}{\\partial \\Theta_1}J(\\Theta)} \\\\\n",
        "{\\frac{\\partial}{\\partial \\Theta_2}J(\\Theta)}\n",
        "\\end{pmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJON936mzAt2"
      },
      "source": [
        "le traitement va devenir repeter {$$ \\Theta = \\Theta  - \\alpha\\triangledown J(\\Theta ) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z7uRcjizh8d"
      },
      "source": [
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGCZIoqdzxiC"
      },
      "source": [
        "Ainsi, on implémente la fonction python qui calcule les gradients et qui recoit en paramètre les éléments suivants: \n",
        "- une matrice mx3.\n",
        "- Θ correspondant aux paramètres de notre modèle.\n",
        "- Un vecteur Y (ou un scalaire yi).\n",
        "\n",
        "cette fonction va retourner le vercteur de gradient $$ \\triangledown J(\\Theta ) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2bQlRrH00F1"
      },
      "source": [
        "def computeGradient(x, y, theta):\n",
        "    h_theta_de_x = h_theta(x,theta)\n",
        "    vec_temp =  (h_theta_de_x - y)\n",
        "    ret = np.dot(x.T,vec_temp)/y.size\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPbiHKVCWBFS"
      },
      "source": [
        "Nous pouvons donc calculer le vecteur des gradients pour voir ce que celà donne :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V1CqIdlWDQx",
        "outputId": "23807404-3a23-4d1d-d142-9569d9abeed9"
      },
      "source": [
        "theta = np.array([0.5, 1, 2])\n",
        "print(\"pour theta = (0.5, 1), le vecteur des gradients est :\", computeGradient(X, Y, theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pour theta = (0.5, 1), le vecteur des gradients est : [-3.38405138e+05 -7.59573941e+08 -1.11367086e+06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_Qm80EWl5H"
      },
      "source": [
        "A l'image de la fonction de calcul de gradient, la fontion d'entrainement du modèle est pareil que pour une régression linéaire avec une seule variable. La fonction sera défini ainsi : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV1nBs90XAve"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "np.seterr(all='warn')\n",
        "np.set_printoptions(suppress=True)\n",
        "def trainModel(x, y, nb_epoch, alpha=0.01):\n",
        "    # on choisi des valeurs arbitraires pour le vecteur theta\n",
        "    theta = np.random.rand(3)\n",
        "    scem = np.zeros((nb_epoch//1000, 2))\n",
        "    print(\"theta \", theta)\n",
        "    # on itere sur le nombre d'epoques\n",
        "    for i in range(nb_epoch+1):\n",
        "        grad = computeGradient(x, y, theta)\n",
        "        theta = theta - alpha*grad\n",
        "        if i%1000 == 0 : \n",
        "            err = computeErrorFunction(x, y, theta)\n",
        "            print(\"theta \", theta)\n",
        "            print(\"gradient :\", grad)\n",
        "            print(\"h_theta : \",h_theta(x,theta))\n",
        "            print(\"Epoque \", i)\n",
        "            print(\"l'erreur est : \", err)\n",
        "            l = (i//1000) - 1 \n",
        "            scem[l, 0] = i\n",
        "            scem[l, 1] = err\n",
        "    print(scem)   \n",
        "    plt.plot(scem[:,0], scem[:,1], linestyle='solid')\n",
        "    plt.show()\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoilH_kqXlil"
      },
      "source": [
        "Entrainons notre modèle (sur 2000 époques par exemple) en éxecutant le code suivant :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yo4UwJ08Xsz4",
        "outputId": "e9a01e58-0b7d-44ae-c977-c0fa53ae2106"
      },
      "source": [
        "theta_train = trainModel(X, Y, 15000, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theta  [0.75311316 0.37622973 0.84192724]\n",
            "theta  [   3397.31833077 7624635.43252821   11179.29603357]\n",
            "gradient : [-3.39656522e+05 -7.62463506e+08 -1.11784541e+06]\n",
            "h_theta :  [1.60422699e+10 1.21994536e+10 1.82991620e+10 1.07965095e+10\n",
            " 2.28739544e+10 1.51349494e+10 1.16962277e+10 1.08803917e+10\n",
            " 1.05220338e+10 1.13912423e+10 1.47918409e+10 1.52493078e+10\n",
            " 1.44105979e+10 3.41431768e+10 9.66807466e+09 1.75367096e+10\n",
            " 1.00645445e+10 9.42408633e+09 1.98927220e+10 2.31103181e+10\n",
            " 1.34727677e+10 1.43953375e+10 1.22299522e+10 1.49595828e+10\n",
            " 2.96598688e+10 8.38713591e+09 1.11167554e+10 1.92598660e+10\n",
            " 1.67742349e+10 2.01062006e+10 1.40217303e+10 7.62465001e+09\n",
            " 1.55543044e+10 2.39185183e+10 1.38082629e+10 1.09566381e+10\n",
            " 9.44696024e+09 1.62557709e+10 3.21378865e+10 1.64845099e+10\n",
            " 1.26874191e+10 1.70639710e+10 1.95724873e+10 9.14959945e+09\n",
            " 6.49621514e+09 1.41208729e+10 9.17247336e+09]\n",
            "Epoque  0\n",
            "l'erreur est :  1.3431116615957727e+20\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  1000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  2000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  3000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  4000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  5000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  6000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  7000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  8000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  9000\n",
            "l'erreur est :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in subtract\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  10000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  11000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  12000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  13000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  14000\n",
            "l'erreur est :  nan\n",
            "theta  [nan nan nan]\n",
            "gradient : [nan nan nan]\n",
            "h_theta :  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan]\n",
            "Epoque  15000\n",
            "l'erreur est :  nan\n",
            "[[ 1000.    nan]\n",
            " [ 2000.    nan]\n",
            " [ 3000.    nan]\n",
            " [ 4000.    nan]\n",
            " [ 5000.    nan]\n",
            " [ 6000.    nan]\n",
            " [ 7000.    nan]\n",
            " [ 8000.    nan]\n",
            " [ 9000.    nan]\n",
            " [10000.    nan]\n",
            " [11000.    nan]\n",
            " [12000.    nan]\n",
            " [13000.    nan]\n",
            " [14000.    nan]\n",
            " [15000.    nan]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW041xQYbyoX"
      },
      "source": [
        "Nous constatons que notre modèle ne converge pas, au contraire l'erreur augmente au fure et à mesure que l'entrainement avance.\n",
        "Pourquoi ce comportement ? \n",
        "c'est ce que nous allons voire dans section suivante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glnNeOvT-Hb-"
      },
      "source": [
        "## 3.2 - Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3-J4pW7tLpr"
      },
      "source": [
        "Nous avons l'impression que l'erreur s'augmente au fure et à mesure que l'entrainement avance alors que ca devrai etre le contraire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ClwPRA-M5v"
      },
      "source": [
        "Cette situation s'explique du fait que le gradient oscille : c'est-à dire qu'elle prend des valeurs (positives ou négatives) avec des valeurs absolus trés grandes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzKsjuaz2AR"
      },
      "source": [
        "Ce phénomène est causé par le fait que les deux variables ne sont pas dans une meme echelle : \n",
        "* la surface $$ x_{1}^{(i)} \\, est \\, de \\, tel \\, sorte\\, que\\, \\, \\, 852 \\leq x_{1}^{(i)}\\leq 4478$$ \n",
        "* le nombre de chambres $$ x_{2}^{(i)} \\, est \\, de \\, tel \\, sorte\\, que\\, \\, \\, 1 \\leq x_{2}^{(i)}\\leq 5 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSy9yKZ04psn"
      },
      "source": [
        "Lors d'une regression lineaire avec plusieurs varables, pour que descente de gradient se passe bien, il faut que tous les caracteristiques (variables) aient des valeurs dans le meme echelle. cela signifie que $$ \\forall \\, x_{j}^{(i)} \\, nous \\, devons \\, avoir \\, \\, \\, -1 \\leq x_{j}^{(i)}\\leq 1\\, \\, ou \\, bien\\, -0,5 \\leq x_{j}^{(i)}\\leq 05 $$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p_QL33N6TjU"
      },
      "source": [
        "Pour avoir cette uniformisation, il nous faudra faire un redimensionnement de caractéristiques (qui est aussi appelé Feature scalling).\n",
        "Le redimensionnement de caractéristiques le plus utilisé est la \"normalisation moyenne\" (mean normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG2qXQie8ujj"
      },
      "source": [
        "La \"normalisation moyenne\" consiste à faire : $$ \\forall \\, x_{j}^{(i)} \\, on \\, fait \\,\\, \\, \\,  x_{j}^{(i)} = \\frac{x_{j}^{(i)} - \\mu _{i}}{s_{i}} \\\\\n",
        "avec \\, \\mu _{i} = moyenne \\, des\\, x_{j}^{(i)}\\\\\n",
        "et \\, s_{i} = \\underset{i}{max}(x_{j}^{(i)}) - \\underset{i}{min}(x_{j}^{(i)}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayfy6yzu_Bui"
      },
      "source": [
        "Maintenant, appliquons la \"normalisation moyenne\" à nos deux caractéristiques à savoire les nombres de chambres et les surfaces des maisons. Pour ce faire, nous allons dèja créer une fonction qui permet de normaliser nos deux caractéristiques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naq7QYTKERMv"
      },
      "source": [
        "mu = np.zeros(np.size(X, 1))\n",
        "s = np.zeros(np.size(X, 1))\n",
        "mu = np.mean(X, axis=0, dtype=int)\n",
        "min = np.min(X, axis=0)\n",
        "max = np.max(X, axis=0)\n",
        "s = max - min\n",
        "\n",
        "def normalize(X, mu, s):\n",
        "    X_norm = X\n",
        "\n",
        "    for i in range(np.size(X_norm,0)) : \n",
        "      X_norm[i,:] = X_norm[i,:] - mu\n",
        "      for j in range(np.size(X_norm,1)) : \n",
        "        X_norm[i,j] = X_norm[i,j]/s[j]\n",
        "\n",
        "    # nous remettons les 1 à leurs places\n",
        "    X_norm[:,0] = np.ones(np.size(X_norm,0),dtype=int)\n",
        "    return X_norm\n",
        "\n",
        "#X_norm = normalize(X, mu, s)\n",
        "#print(mu)\n",
        "#print(min)\n",
        "#print(max)\n",
        "#print(s)\n",
        "#print(X_norm[0:9,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqGM053JOK7B",
        "outputId": "08b384fc-0b42-47dc-df2c-f5022138ecf7"
      },
      "source": [
        "X_norm = normalize(X, mu, s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wezCRvi_S2oN",
        "outputId": "8bb90780-ab01-4fcd-aa39-668ddb79ecee"
      },
      "source": [
        "theta_train = trainModel(X_norm, Y, 15000, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theta  [0.92497256 0.30792978 0.60657798]\n",
            "theta  [3405.04205988  230.26850794  248.43019664]\n",
            "gradient : [-340411.70873226  -22996.05781648  -24782.36186608]\n",
            "h_theta :  [3411.64656204 3379.6401285  3430.44399126 3305.84769091 3530.65443749\n",
            " 3466.19703662 3375.44880983 3368.65379318 3365.66906625 3372.90861669\n",
            " 3463.33931934 3405.04205988 3398.05652875 3686.62212309 3358.55652546\n",
            " 3486.20105758 3299.75122738 3356.52437095 3505.82404957 3532.62308717\n",
            " 3390.24543485 3335.82196993 3379.89414782 3464.73642556 3525.06618564\n",
            " 3347.88771428 3370.62244286 3438.44559964 3417.74302557 3445.4946356\n",
            " 3332.71023334 3217.32213311 3469.68980218 3477.24704982 3455.14719647\n",
            " 3369.28884147 3356.71488543 3475.5322464  3607.81280405 3477.43739125\n",
            " 3321.59688836 3420.15620905 3503.15684677 3354.23819712 3270.03096767\n",
            " 3457.75089443 3354.42871161]\n",
            "Epoque  0\n",
            "l'erreur est :  64426813706.638954\n",
            "theta  [337221.03825946 173547.3435808   77978.38825689]\n",
            "gradient : [   159.24533617 -12998.24962174  -3566.18662974]\n",
            "h_theta :  [342198.67856073 318076.26786997 356365.80864896 289775.07642658\n",
            " 404577.56129743 355997.70643408 314917.3807557  309796.15467651\n",
            " 307546.64415574 313002.90371675 353843.91976526 337221.03825946\n",
            " 331956.22640235 494812.08495085 302186.10844668 371074.21311581\n",
            " 285180.3315331  300654.52681552 385863.5482417  406061.28100262\n",
            " 326069.20950758 312365.90548618 318267.71557386 354896.88213669\n",
            " 427680.07834984 294145.30488309 311279.8743817  362396.41132165\n",
            " 346793.42345421 367709.08510474 310020.67111347 250369.91815727\n",
            " 358630.11236264 391640.04809161 347669.73131465 310274.77393625\n",
            " 300798.11259345 363033.40955222 462729.80135553 364469.26733143\n",
            " 301644.83406806 348612.17664122 383853.3473508  298931.49748047\n",
            " 262780.95017738 349632.07027957 299075.08325839]\n",
            "Epoque  1000\n",
            "l'erreur est :  3993350281.8959403\n",
            "theta  [336274.87653911 274915.09083731  96858.16476863]\n",
            "gradient : [   35.55395645 -7798.16508342  -584.0229195 ]\n",
            "h_theta :  [344159.92051238 305947.78433422 366601.968744   267782.78072781\n",
            " 436307.14824348 359352.15177358 300943.81412041 292831.31695561\n",
            " 289267.88362153 297911.10489992 355940.35390053 336274.87653911\n",
            " 327934.92618276 572580.2951327  280776.29780416 383234.73688493\n",
            " 260504.27859864 278350.13042777 406662.41561321 438657.49788936\n",
            " 318609.34532976 303568.74952958 306251.05525627 357608.3439718\n",
            " 479570.3872072  268038.91907811 295181.66660148 376155.00278854\n",
            " 351438.42264155 384570.77087539 299853.68073448 212028.06364257\n",
            " 363522.12695175 422479.6361315  346159.86666445 293589.49426073\n",
            " 278577.58361931 370497.35815888 528425.69081583 372771.89007424\n",
            " 286585.57789484 354319.49640102 403478.07093169 275620.69212933\n",
            " 225021.58071892 349268.39361546 275848.14532087]\n",
            "Epoque  2000\n",
            "l'erreur est :  2900640104.287359\n",
            "theta  [336279.5464629  337093.96595764  94986.83614484]\n",
            "gradient : [  -27.78405899 -4916.09086706   769.00805636]\n",
            "h_theta :  [345947.98895038 299093.22920337 373465.86372243 258240.81422778\n",
            " 452992.04864793 358631.76860188 292957.48685555 283010.14698862\n",
            " 278640.75471063 289238.85512959 354448.30791018 336279.5464629\n",
            " 326053.30921653 614142.1999581  268228.58587796 387915.99344376\n",
            " 249316.09808549 265253.6804972  416642.42352674 455873.98823555\n",
            " 314618.51665922 302120.66859402 299465.09237596 356493.55535945\n",
            " 511984.89551418 252610.33262896 285892.08657624 385179.55365918\n",
            " 354872.70509266 395498.7566987  297565.34472973 195820.33524166\n",
            " 363744.88722506 441981.65327311 342455.72059398 283939.80492011\n",
            " 265532.57787664 372297.74019475 565945.48732375 375086.71398922\n",
            " 281296.33092868 358405.40523232 412737.86021449 261906.91194384\n",
            " 205808.10689184 346267.31811308 262185.80932329]\n",
            "Epoque  3000\n",
            "l'erreur est :  2505369835.3550024\n",
            "theta  [336707.25938799 377290.19041853  84156.11300374]\n",
            "gradient : [  -53.43135984 -3272.97362007  1303.36348704]\n",
            "h_theta :  [347528.59965372 295086.71990443 378327.79887156 254902.24349105\n",
            " 461797.63634784 356185.51740829 288219.33088964 277085.83657779\n",
            " 272195.42318847 284057.27694128 351503.20671639 336707.25938799\n",
            " 325261.61103001 636624.55799055 260541.67213307 388961.6922516\n",
            " 244913.314015   257212.02897438 421113.55900266 465023.22815782\n",
            " 312463.29513882 304014.48008166 295502.92529926 353792.33638799\n",
            " 533364.30844784 243061.04554997 280311.42838776 391438.26880888\n",
            " 357517.52912978 402987.96851557 298915.96399492 190577.85417721\n",
            " 361908.34158728 455013.64287003 338080.58273294 278126.35006487\n",
            " 257524.18302051 371481.0656685  588220.02502918 374602.60612977\n",
            " 280706.97797086 361471.48038072 416743.40235688 253466.18042086\n",
            " 196217.28281922 342346.68803001 253778.33446699]\n",
            "Epoque  4000\n",
            "l'erreur est :  2329606788.430323\n",
            "theta  [337287.61948023 404745.80515336  70229.10280491]\n",
            "gradient : [  -60.56717982 -2299.98199323  1437.38979671]\n",
            "h_theta :  [348896.4346308  292638.3304396  381936.90852087 254542.38177968\n",
            " 466468.11778305 353170.54684244 285271.19774789 273327.51292952\n",
            " 268081.22146725 280806.26884383 348147.50182537 337287.61948023\n",
            " 325009.06499406 649004.51648943 255579.42053587 388331.86196194\n",
            " 243826.55240993 252007.47741262 422823.43774583 469928.4376837\n",
            " 311279.40861406 307228.54284763 293084.82333001 350603.2127226\n",
            " 548255.51019724 236826.7191388  276787.83283017 396001.43456867\n",
            " 359612.26400055 408391.61227745 301759.00494015 190549.84547619\n",
            " 359309.82408553 464203.22357824 333748.10610976 274443.74515554\n",
            " 252342.34708042 369579.16056487 602090.33324398 372927.85724292\n",
            " 282224.94098487 363853.94645941 418135.26239656 247989.04139896\n",
            " 191586.88423238 338324.65823643 248323.91106677]\n",
            "Epoque  5000\n",
            "l'erreur est :  2234009259.2956834\n",
            "theta  [337890.24063231 424496.89775385  56016.5604839 ]\n",
            "gradient : [  -59.03202776 -1695.93837253  1385.4609877 ]\n",
            "h_theta :  [350065.55154417 291062.12174055 384718.35952408 255517.04692936\n",
            " 468964.6779827  350138.32629485 283335.48212341 270808.96031986\n",
            " 265306.65635008 278652.67023423 344870.16291952 337890.24063231\n",
            " 325012.50793708 655998.71740875 252194.78306038 387015.46992211\n",
            " 244278.29839534 248448.53354904 423190.191766   472593.85719681\n",
            " 310612.86137786 310774.22722164 291530.40292947 347445.70945857\n",
            " 559153.1023959  232526.97312584 274438.13953397 399469.21697498\n",
            " 361304.3000782  412464.01996745 305037.7826574  192811.66316095\n",
            " 356577.19264247 470999.16858215 329768.09457693 271979.66329215\n",
            " 248799.74444073 367347.65998757 611205.08911644 370859.76890445\n",
            " 284550.48064225 365752.97137291 418273.23928237 244234.00284878\n",
            " 189489.39929197 334567.97676334 244585.21374047]\n",
            "Epoque  6000\n",
            "l'erreur est :  2174464181.743177\n",
            "theta  [338456.20560192 439346.38869575  42773.60295261]\n",
            "gradient : [  -53.81104692 -1300.28991204  1255.95112004]\n",
            "h_theta :  [351057.42579618 289989.97408557 386922.43711828 257002.10684989\n",
            " 470315.18513096 347332.12265821 281993.04588537 269028.32895475\n",
            " 263333.54675157 277146.42273373 341879.67161262 338456.20560192\n",
            " 325127.99193493 660091.31132204 249763.00192699 385499.27997734\n",
            " 245370.21128597 245885.70340569 422939.44382373 474071.31807348\n",
            " 310224.62574365 314192.26003919 290474.63640073 344545.31434602\n",
            " 567459.1495167  229407.18469013 272784.46189726 402189.30004593\n",
            " 362689.3213601  415638.67929172 308255.14667844 195903.82533473\n",
            " 353996.22949171 476221.46868716 326249.3119486  270239.98474265\n",
            " 246249.20014206 365143.46274047 617531.36336189 368778.4301042\n",
            " 287051.17039003 367293.61335415 417850.48951451 241523.74256921\n",
            " 188664.72041183 331217.10067902 241887.23930559]\n",
            "Epoque  7000\n",
            "l'erreur est :  2134703601.4619296\n",
            "theta  [338962.49027438 450901.55310526  30976.17798454]\n",
            "gradient : [  -47.37523082 -1026.54606495  1102.38194721]\n",
            "h_theta :  [351895.13272418 289221.55777518 388703.42277358 258596.68432941\n",
            " 471058.86601852 344841.2498018  281014.30391281 267708.60446928\n",
            " 261864.04490062 276040.21066289 339245.39489564 338962.49027438\n",
            " 325283.7338371  662595.65609921 247936.58380084 384012.23414492\n",
            " 246658.86052961 243957.30920091 422437.10450055 474913.78828721\n",
            " 309988.3970936  317290.98467847 289718.96710017 341981.14618309\n",
            " 573988.39633311 227045.39215118 271563.52673797 404371.81651083\n",
            " 363832.95652398 418174.92527936 311197.72044732 199122.07003411\n",
            " 351680.62802044 480351.09090336 323203.94416465 268952.12778176\n",
            " 244330.36619465 363121.04249525 622146.94848485 366851.61243269\n",
            " 289436.06247892 368558.34511141 417214.30658814 239480.62527598\n",
            " 188461.96950554 328302.38974581 239853.68226972]\n",
            "Epoque  8000\n",
            "l'erreur est :  2107328369.9765391\n",
            "theta  [339403.59104361 460121.30302769  20721.73567333]\n",
            "gradient : [ -40.90222599 -827.3712792   950.08767061]\n",
            "h_theta :  [352600.67199091 288645.58740019 390161.59468704 260116.47180587\n",
            " 471479.03407052 342680.59982532 280270.51679902 266692.7508244\n",
            " 260728.6853963  275194.71643468 336970.32441543 339403.59104361\n",
            " 325445.14004167 664210.29145133 246516.44437614 382652.52769452\n",
            " 247934.55093145 242455.80408466 421863.08550907 475412.77935289\n",
            " 309837.05392132 320010.91610512 289153.16743662 339762.01461582\n",
            " 579235.15825882 225198.0828459  270626.49610677 406150.36583472\n",
            " 364782.59286533 420235.71184578 313793.0606588  202147.71409837\n",
            " 349659.82532629 483683.21640006 320600.86824043 267961.70091549\n",
            " 242836.48911199 361334.16616428 625656.47013744 365141.01643754\n",
            " 291586.4340648  369604.60321146 416533.49512651 237887.58375676\n",
            " 188547.68666864 325803.56361388 238268.26878408]\n",
            "Epoque  9000\n",
            "l'erreur est :  2088242518.9488096\n",
            "theta  [339782.11478455 467606.23467054  11934.16640124]\n",
            "gradient : [ -34.91154876 -676.25435491  810.08458114]\n",
            "h_theta :  [353193.87661735 288198.41542763 391365.81414148 261486.37212313\n",
            " 471724.90477718 340831.26765898 279687.10503373 265888.46545576\n",
            " 259827.38078132 274528.73509804 335028.10148132 339782.11478455\n",
            " 325596.5974614  665310.21550133 245383.94496138 381453.43090256\n",
            " 249106.28427747 241257.24901282 421301.83865578 475722.64147734\n",
            " 309734.60990914 322355.13736431 288714.2524212  337865.20494595\n",
            " 583515.09424603 223718.79123147 269886.20215592 407614.67943891\n",
            " 365573.96446302 421929.15601046 316036.13419308 204855.78319162\n",
            " 347924.02632055 486408.78020662 318392.35843871 267178.05793968\n",
            " 241644.126758   359788.27717265 628410.39157384 363657.05462442\n",
            " 293468.26572443 370474.41590192 415885.55022331 236614.7160707\n",
            " 188753.35602987 323679.6876228  237001.59381588]\n",
            "Epoque  10000\n",
            "l'erreur est :  2074869455.0604978\n",
            "theta  [340104.0257456  473753.47914468   4467.08029625]\n",
            "gradient : [ -29.59396317 -557.86232351  686.25885205]\n",
            "h_theta :  [353692.10126436 287842.19682726 392365.85466393 262684.98545077\n",
            " 471875.36811549 339260.97723522 279218.99505574 265238.95582009\n",
            " 259098.19092218 273992.81216391 333381.52148191 340104.0257456\n",
            " 325732.02279305 666099.59604279 244464.87882505 380417.16750841\n",
            " 250142.14651037 240283.93251158 420789.43034782 475925.65985666\n",
            " 309661.51040067 324353.9435744  288364.81511645 336255.92207242\n",
            " 587041.16738472 222514.91067935 269289.24756126 408828.3307732\n",
            " 366234.94020476 423330.98829804 317951.8695319  207215.91330164\n",
            " 346446.97871149 488658.27444596 316527.08165575 266545.50154304\n",
            " 240675.89622847 358467.19936271 630620.67345493 362386.83653158\n",
            " 295087.31938013 371199.813952   415301.9383114  235580.36790893\n",
            " 188995.80667592 321883.91911988 235972.33162582]\n",
            "Epoque  11000\n",
            "l'erreur est :  2065480859.8472211\n",
            "theta  [340376.301655   478840.31162707  -1845.46850629]\n",
            "gradient : [ -24.98130922 -462.94744067  579.06919666]\n",
            "h_theta :  [354110.27639554 287553.32188367 393199.28142632 263716.11831543\n",
            " 471972.38395674 337934.072787   278837.5302214  264707.38313257\n",
            " 258500.68300944 273555.23224427 331991.48756272 340376.301655\n",
            " 325849.98221788 666691.92708521 243710.24867347 379532.16935692\n",
            " 251038.60317032 239484.41029176 420337.92123027 476066.16488901\n",
            " 309606.9159382  326047.2344456  288081.55168138 334896.75145015\n",
            " 589964.88107451 221524.59716951 268801.16406485 409838.52005429\n",
            " 366787.79154066 424496.89694083 319576.41942361 209241.58647983\n",
            " 345197.23250555 490525.62165499 314956.07658647 266027.95762685\n",
            " 239880.58264005 357346.51785296 632422.18501214 361308.24133581\n",
            " 296466.36577366 371805.97461893 414791.50835428 234730.34211234\n",
            " 189235.71683786 320370.43201303 235126.51446063]\n",
            "Epoque  12000\n",
            "l'erreur est :  2058884527.0720627\n",
            "theta  [340605.82734627 483070.06498707  -7165.22187402]\n",
            "gradient : [ -21.03325176 -385.64403575  487.43531255]\n",
            "h_theta :  [354461.1187855  287316.24488768 393895.40980485 264594.34242524\n",
            " 472038.47802422 336816.16253556 278523.46378201 264268.50047434\n",
            " 258006.97453546 273194.50553616 330821.08450897 340605.82734627\n",
            " 325951.19217015 667152.17974019 243085.89144705 378781.7087217\n",
            " 251804.84263518 238822.72485037 419947.91117096 476168.42066476\n",
            " 309564.64556414 327476.04972637 287849.14071227 333752.0115442\n",
            " 592399.10446308 220704.26681445 268398.44311488 410681.62827931\n",
            " 367250.61857556 425469.48741156 320948.07587519 210964.48213681\n",
            " 344143.48012362 492081.46548479 313635.19416608 265600.74003581\n",
            " 239222.39671881 356400.08408909 633905.58474218 360396.80277349\n",
            " 297633.88354956 372313.12890912 414352.50501281 234026.6624291\n",
            " 189456.03115863 319097.37636808 234426.33429753]\n",
            "Epoque  13000\n",
            "l'erreur est :  2054248626.9073977\n",
            "theta  [340798.91653081 486597.96888435 -11639.61765829]\n",
            "gradient : [ -17.68089846 -322.0220629   409.68492786]\n",
            "h_theta :  [354755.39440283 287120.15548455 394477.67757707 265337.82981784\n",
            " 472085.91473189 335876.058577   278263.15991191 263904.09133204\n",
            " 257596.8369091  272895.28380729 329837.19795929 340798.91653081\n",
            " 326037.25724309 667519.03238325 242566.78381615 378148.08290093\n",
            " 252454.92716674 238272.48293245 419614.92580917 476246.01871297\n",
            " 309531.03822136 328678.76785243 287656.94309501 332789.52981684\n",
            " 594431.06247439 220021.70417672 268064.19531312 411386.48730664\n",
            " 367638.29705394 426282.34349698 322103.11962426 212421.8227443\n",
            " 343256.88822086 493380.7948048  312525.79752188 265246.06035819\n",
            " 238675.0736403  355603.0032615  635135.1514099  359628.91033997\n",
            " 298618.66166652 372737.77935333 413978.65589931 233441.39443829\n",
            " 189650.77674261 318027.87052912 233843.98514613]\n",
            "Epoque  14000\n",
            "l'erreur est :  2050990139.181408\n",
            "theta  [340961.14598819 489546.19136934 -15398.46056011]\n",
            "gradient : [ -14.84813561 -269.3034125   344.01488681]\n",
            "h_theta :  [355002.18401974 286957.15355914 394965.13841724 265964.9321818\n",
            " 472121.51192079 335086.38113207 278046.49480835 263600.42683357\n",
            " 257254.95772316 272646.09556544 329010.9319838  340961.14598819\n",
            " 326110.0480702  667816.6488061  242133.83984303 377614.52516995\n",
            " 253003.97399883 237813.5204487  419332.60932139 476306.82133404\n",
            " 309503.82039827 329689.64324808 287497.19348343 331981.1515674\n",
            " 596130.01021545 219452.16302283 267785.73624683 411976.39603239\n",
            " 367963.14220271 426962.50393145 323074.15417552 213650.39519562\n",
            " 342511.93009107 494467.49446776 311594.64442544 264950.5266443\n",
            " 238218.55039192 354932.84834975 636158.63892403 358983.14778193\n",
            " 299447.40748781 373093.52148347 413662.19011634 232953.16113009\n",
            " 189819.30285684 317130.05364941 233358.19107331]\n",
            "Epoque  15000\n",
            "l'erreur est :  2048699705.2035973\n",
            "[[1.00000000e+03 3.99335028e+09]\n",
            " [2.00000000e+03 2.90064010e+09]\n",
            " [3.00000000e+03 2.50536984e+09]\n",
            " [4.00000000e+03 2.32960679e+09]\n",
            " [5.00000000e+03 2.23400926e+09]\n",
            " [6.00000000e+03 2.17446418e+09]\n",
            " [7.00000000e+03 2.13470360e+09]\n",
            " [8.00000000e+03 2.10732837e+09]\n",
            " [9.00000000e+03 2.08824252e+09]\n",
            " [1.00000000e+04 2.07486946e+09]\n",
            " [1.10000000e+04 2.06548086e+09]\n",
            " [1.20000000e+04 2.05888453e+09]\n",
            " [1.30000000e+04 2.05424863e+09]\n",
            " [1.40000000e+04 2.05099014e+09]\n",
            " [1.50000000e+04 2.04869971e+09]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHa1Le1c-Non"
      },
      "source": [
        "# 4 - Regréssion polynomiale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0456Ty87-cZI"
      },
      "source": [
        "Lorsqu'on effectue une régression lineaire, il peut arriver que le modèle linéaire qu'on a appris ne puisse pas pemettre de faire les prédiction attendu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lgu4tk-xXvY"
      },
      "source": [
        "En visualisant les données, on se rend compte que l'affiche des données prend la forme d'une courbe au lieu d'une droite.\n",
        "sachant qu'une fonction linéaire ne peut représenter qu'une droite; un modeèle linéaire ne peut pas apprendre nos données correctement (c'est ce qui explique les disfonctionnements de notre modèle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8K9z4lzV4D"
      },
      "source": [
        "pour apprendre un modèle à partir de ces données (sous forme de courbe), il nous faut faire une regréssion polynomiale en lieu et place de la regréssion linéaire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_7r1usA1xaX"
      },
      "source": [
        "la régression polynomiale consiste à modifier fonction linéaire en fonction polynamiale en créant de nouvelle caractéristiques à partie des caractéristiquers existants.\n",
        "cette modification (transfomation) nous donnera une fonction quadratique, cubique, racine carrée, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsDREEJy5ryx"
      },
      "source": [
        "Par exemple, supposons qu'on ait le modèle de regréssion suivant : $$ h_{\\Theta} (x) = \\Theta _{0} + \\Theta _{1}x_{1}  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnwXB5GpVeNc"
      },
      "source": [
        "nous pouvons créer des caractéristiques additionnelles à partir de x. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioh4_7w2VtpS"
      },
      "source": [
        "Pour avoir la fonction quadratique, on fait : $$ h_{\\Theta} (x) = \\Theta _{0} + \\Theta _{1}x_{1}  + \\Theta _{2}(x_{1})^2$$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj34hEqLWYha"
      },
      "source": [
        "Pour avoir la fonction cubique, on fait : $$ h_{\\Theta} (x) = \\Theta _{0} + \\Theta _{1}x_{1}  + \\Theta _{2}(x_{1})^2 + \\Theta _{3}(x_{1})^3$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6ZiC8aUWy_H"
      },
      "source": [
        "Dans le modèle cubique nous avons créer deux nouveaux caractéristiques qui sont : $$ x_{2} = (x_{1})^2 \\,\\, et \\,\\, x_{3} = (x_{1})^3$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jcpcl1L5IaJ"
      },
      "source": [
        "Dans une regréssion polynomiale, il faut toujours faire du  redimensionnement de caractéristiques (Feature scalling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHZsRMLI-dFW"
      },
      "source": [
        "# 5 - Equation normale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlGKOEV5-ikX"
      },
      "source": [
        "La descente de gradient est un moyen de minimiser le fonction d'erreur J_theta.\n",
        "Cependant, il existe une seconde manière de minimiser la fonction d'erreur. cette manière de faire est appelé Equation Normale. Cette manière de faire permet d'effectuer la minimisation explicitement et sans recourir à un algorithme itératif. Dans la méthode \"Équation normale\", nous minimiserons J en prenant explicitement ses dérivés par rapport aux θj, et en les mettant à zéro. Cela nous permet de trouver le thêta optimal sans itération. \n",
        "La formule d’équation normale est donnée ci-dessous :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kgnxXHU-Is"
      },
      "source": [
        "$$ \\theta  = (X^{T}X)^{-1}X^{T}y $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q2k3bz7Vi-E"
      },
      "source": [
        "*   X est une matrice mxn qui est composé par l'ensemble des exemples d'entrainement sans les target : dans notre cas, chaque ligne représente un maison.\n",
        "*   y est un vecteur de dimesion m qui représente l'ensemble des target : chaque élément de ce vecteur représente le prix d'une maison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAwvVB_sWr9H"
      },
      "source": [
        "Nous pouvons définir la fonction qui calcule theta en utilisant l'equation normale. \n",
        "cette fonction va recevoire les paramètres suivants : \n",
        "- une matrice mx3.\n",
        "- Un vecteur y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otxXWDhFTNXE"
      },
      "source": [
        "def equationNormale(X, y):\n",
        "    matM = np.dot(X.T, X)\n",
        "    matInv = np.linalg.inv(matM)\n",
        "    mat = np.dot(matInv, X.T)\n",
        "    theta = np.dot(mat, y)\n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmw-d3ILZGqQ",
        "outputId": "05a2b09d-c19e-4e69-8194-15104ab81dd3"
      },
      "source": [
        "theta = equationNormale(X, Y)\n",
        "\n",
        "print(theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[341805.20024107 504777.90398791 -34952.07644931]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wnj8xrTZfkH"
      },
      "source": [
        "avec l'equation Normale, on a pas besoin de choisir un learning rate ni de faire plusieurs itération comme on le ferait sur un descente de gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIw1ph02aC6j"
      },
      "source": [
        "Par contre, si on a un nombre de feature trés grand (>10000), faire une descente de gradient ira beaucoup plus vite que de calculer l'équation normale.\n",
        "En plus, l'equation normale ne fonctionne que pour la regréssion linéaire mais pas pour les autres algorithme plus complexes (classification, regression logistique, reseau de neurone, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgM70PhqbO2_"
      },
      "source": [
        "# 6 - Conclusion"
      ]
    }
  ]
}